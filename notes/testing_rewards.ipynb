{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datapoint = namedtuple(\"Datapoint\", (\"network_input\", \"action\", \"discounted_reward\"))\n",
    "with open(\"../agent_code/training_data/training_data_cleaned.pt\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([d.discounted_reward for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 0.1653856406661175, 0.47785096305970176)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(rewards), np.min(rewards), np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23906, 221310, 138565)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(rewards == 0), np.count_nonzero(rewards > 0), np.count_nonzero(rewards < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>383781.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.165386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.477852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.038170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.041675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.471077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count  383781.000000\n",
       "mean        0.165386\n",
       "std         0.477852\n",
       "min        -1.000000\n",
       "25%        -0.038170\n",
       "50%         0.041675\n",
       "75%         0.471077\n",
       "max         1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(rewards).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now testing a different reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\") # NamedTuple definition\n",
    "with open(\"../agent_code/training_data/training_data_solo.pt\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [d.reward for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import events as e\n",
    "\n",
    "def get_reward_from_events(events):\n",
    "    rewards = {\n",
    "        e.CRATE_DESTROYED: 0.1, # incentivize destroying crates\n",
    "        e.KILLED_SELF: -1,\n",
    "        e.GOT_KILLED: -4, # KILLED_SELF + GOT_KILLED = -4\n",
    "        e.INVALID_ACTION: -0.1, # WAIT instead\n",
    "        e.COIN_COLLECTED: 1,\n",
    "        e.KILLED_OPPONENT: 5, # Killing an opponent at the same time as getting killed is a net reward of 1\n",
    "    }\n",
    "    reward = sum([rewards.get(event, 0.) for event in events])\n",
    "    return round(reward, 2) # .2 + .1 shenanigans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([get_reward_from_events(event) for event in events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40061, 1.7, -0.1, 0.04989141559122339, 0.1757944043214819)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards), np.max(rewards), np.min(rewards), np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35310, 4593, 158)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(rewards == 0), np.count_nonzero(rewards > 0), np.count_nonzero(rewards < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.93\n",
    "score_reduction_factor = 1 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories.reverse()\n",
    "rewards_clean = np.zeros(len(memories))\n",
    "rewards = np.zeros(len(memories))\n",
    "\n",
    "running_reward = 0\n",
    "for idx, memory in enumerate(memories):\n",
    "    if memory.next_state is None:\n",
    "        running_reward = 0\n",
    "    \n",
    "    reward = get_reward_from_events(memory.reward)\n",
    "\n",
    "    last_rewards = []\n",
    "    for i in range(1, 11):\n",
    "        if memories[idx - i].next_state is None:\n",
    "            break\n",
    "        if idx - i < 0:\n",
    "            break\n",
    "        last_rewards.append(rewards_clean[idx - i] * discount_factor ** i) \n",
    "\n",
    "    running_reward = reward + sum(last_rewards)\n",
    "\n",
    "    bounded_running_reward = min(1, max(-1, running_reward))\n",
    "\n",
    "    rewards_clean[idx] = reward\n",
    "    rewards[idx] = bounded_running_reward\n",
    "\n",
    "memories.reverse()\n",
    "rewards = np.flip(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40061, 1.0, -0.03982991293924298, 0.3656755675369735, 0.31417155660068624)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards), np.max(rewards), np.min(rewards), np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07443866, 0.08004157, 0.0860662 , 0.18934076, 0.20359222,\n",
       "       0.21891636, 0.02034017, 0.12939804, 0.13913767, 0.1496104 ,\n",
       "       0.1608714 , 0.41497115, 0.44620554, 0.4797909 , 0.30085044,\n",
       "       0.32349509, 0.34784418, 0.37402601, 0.4021785 , 0.43245   ,\n",
       "       0.65859292, 0.70816443, 0.22383272, 0.24068035, 0.25879607,\n",
       "       0.27827535, 0.2992208 , 0.3217428 , 0.34596   , 0.372     ,\n",
       "       0.4       , 0.        , 0.14519469, 0.15612332, 0.16787454,\n",
       "       0.18051026, 0.67807936, 0.72911759, 0.78399741, 0.84300797,\n",
       "       0.90646018, 0.97468837, 1.        , 0.94955169, 1.        ,\n",
       "       1.        , 1.        , 0.19409706, 0.20870651, 0.2244156 ])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INVALID_ACTION']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories[6].reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
